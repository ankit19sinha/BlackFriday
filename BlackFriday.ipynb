{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis:\n",
    "\n",
    "For this problem, we have a dataset of over 500,000 observations. We shall use 70% of this for the training, and 30% for testing. The goal here is to use XGBoost and make a basic model first and then tune the parameters to lower our prediction error.\n",
    "\n",
    "Before we begin, we must look for NAs and try to understand the nature of the data we have. For this we summarize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing xgboost\n",
    "library(xgboost)\n",
    "\n",
    "#Setting the path to the directory for data\n",
    "path = \"/home/ankit19/Desktop/Jupyter_Notebooks/Black Friday/data\"\n",
    "setwd(path)\n",
    "\n",
    "#reading training and testing data\n",
    "purchaseData = read.csv(\"./train.csv\", stringsAsFactors = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try and understand the type of data, and their nature that each column has for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    User_ID         Product_ID           Gender              Age           \n",
       " Min.   :1000001   Length:550068      Length:550068      Length:550068     \n",
       " 1st Qu.:1001516   Class :character   Class :character   Class :character  \n",
       " Median :1003077   Mode  :character   Mode  :character   Mode  :character  \n",
       " Mean   :1003029                                                           \n",
       " 3rd Qu.:1004478                                                           \n",
       " Max.   :1006040                                                           \n",
       "                                                                           \n",
       "   Occupation     City_Category      Stay_In_Current_City_Years\n",
       " Min.   : 0.000   Length:550068      Length:550068             \n",
       " 1st Qu.: 2.000   Class :character   Class :character          \n",
       " Median : 7.000   Mode  :character   Mode  :character          \n",
       " Mean   : 8.077                                                \n",
       " 3rd Qu.:14.000                                                \n",
       " Max.   :20.000                                                \n",
       "                                                               \n",
       " Marital_Status   Product_Category_1 Product_Category_2 Product_Category_3\n",
       " Min.   :0.0000   Min.   : 1.000     Min.   : 2.00      Min.   : 3.0      \n",
       " 1st Qu.:0.0000   1st Qu.: 1.000     1st Qu.: 5.00      1st Qu.: 9.0      \n",
       " Median :0.0000   Median : 5.000     Median : 9.00      Median :14.0      \n",
       " Mean   :0.4097   Mean   : 5.404     Mean   : 9.84      Mean   :12.7      \n",
       " 3rd Qu.:1.0000   3rd Qu.: 8.000     3rd Qu.:15.00      3rd Qu.:16.0      \n",
       " Max.   :1.0000   Max.   :20.000     Max.   :18.00      Max.   :18.0      \n",
       "                                     NA's   :173638     NA's   :383247    \n",
       "    Purchase    \n",
       " Min.   :   12  \n",
       " 1st Qu.: 5823  \n",
       " Median : 8047  \n",
       " Mean   : 9264  \n",
       " 3rd Qu.:12054  \n",
       " Max.   :23961  \n",
       "                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(purchaseData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that besides the \"Product_Category_2\" and \"Product_Category_3\" we do not have any other columns with missing values. We also notice that the columns \"Age\" and \"Stay_In_Current_City_Years\" are character types instead of numeric type so before we use XGBoost we will have to address those too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "We shall first address the issue of missing values from the data. Here we'll use the basic approach of replacing NA with median value of the column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace every NA observed in Product_Category_2 of training data with median\n",
    "purchaseData$Product_Category_2[is.na(purchaseData$Product_Category_2)] = median(purchaseData$Product_Category_2, na.rm=T)\n",
    "\n",
    "#Replace every NA observed in Product_Category_3 of training data with median\n",
    "purchaseData$Product_Category_3[is.na(purchaseData$Product_Category_3)] = median(purchaseData$Product_Category_3, na.rm=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed NAs let's shuffle the data to get a more randomized set of observations when we split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>User_ID</th><th scope=col>Product_ID</th><th scope=col>Gender</th><th scope=col>Age</th><th scope=col>Occupation</th><th scope=col>City_Category</th><th scope=col>Stay_In_Current_City_Years</th><th scope=col>Marital_Status</th><th scope=col>Product_Category_1</th><th scope=col>Product_Category_2</th><th scope=col>Product_Category_3</th><th scope=col>Purchase</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>214016</th><td>1003056  </td><td>P00034442</td><td>M        </td><td>26-35    </td><td>18       </td><td>C        </td><td>1        </td><td>1        </td><td>1        </td><td>14       </td><td>16       </td><td>15345    </td></tr>\n",
       "\t<tr><th scope=row>320723</th><td>1001396  </td><td>P00281542</td><td>F        </td><td>26-35    </td><td>12       </td><td>A        </td><td>4+       </td><td>1        </td><td>5        </td><td> 9       </td><td>14       </td><td> 7092    </td></tr>\n",
       "\t<tr><th scope=row>52073</th><td>1001942  </td><td>P00153742</td><td>M        </td><td>36-45    </td><td> 5       </td><td>B        </td><td>1        </td><td>1        </td><td>8        </td><td> 9       </td><td>14       </td><td>10005    </td></tr>\n",
       "\t<tr><th scope=row>469003</th><td>1000235  </td><td>P0094542 </td><td>M        </td><td>26-35    </td><td> 0       </td><td>B        </td><td>0        </td><td>0        </td><td>2        </td><td> 4       </td><td> 9       </td><td>13074    </td></tr>\n",
       "\t<tr><th scope=row>432762</th><td>1000647  </td><td>P00213342</td><td>M        </td><td>36-45    </td><td>12       </td><td>B        </td><td>1        </td><td>1        </td><td>5        </td><td> 9       </td><td>14       </td><td> 7091    </td></tr>\n",
       "\t<tr><th scope=row>65646</th><td>1004048  </td><td>P00220242</td><td>F        </td><td>36-45    </td><td> 1       </td><td>B        </td><td>1        </td><td>0        </td><td>3        </td><td>12       </td><td>14       </td><td>13251    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       "  & User\\_ID & Product\\_ID & Gender & Age & Occupation & City\\_Category & Stay\\_In\\_Current\\_City\\_Years & Marital\\_Status & Product\\_Category\\_1 & Product\\_Category\\_2 & Product\\_Category\\_3 & Purchase\\\\\n",
       "\\hline\n",
       "\t214016 & 1003056   & P00034442 & M         & 26-35     & 18        & C         & 1         & 1         & 1         & 14        & 16        & 15345    \\\\\n",
       "\t320723 & 1001396   & P00281542 & F         & 26-35     & 12        & A         & 4+        & 1         & 5         &  9        & 14        &  7092    \\\\\n",
       "\t52073 & 1001942   & P00153742 & M         & 36-45     &  5        & B         & 1         & 1         & 8         &  9        & 14        & 10005    \\\\\n",
       "\t469003 & 1000235   & P0094542  & M         & 26-35     &  0        & B         & 0         & 0         & 2         &  4        &  9        & 13074    \\\\\n",
       "\t432762 & 1000647   & P00213342 & M         & 36-45     & 12        & B         & 1         & 1         & 5         &  9        & 14        &  7091    \\\\\n",
       "\t65646 & 1004048   & P00220242 & F         & 36-45     &  1        & B         & 1         & 0         & 3         & 12        & 14        & 13251    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | User_ID | Product_ID | Gender | Age | Occupation | City_Category | Stay_In_Current_City_Years | Marital_Status | Product_Category_1 | Product_Category_2 | Product_Category_3 | Purchase | \n",
       "|---|---|---|---|---|---|\n",
       "| 214016 | 1003056   | P00034442 | M         | 26-35     | 18        | C         | 1         | 1         | 1         | 14        | 16        | 15345     | \n",
       "| 320723 | 1001396   | P00281542 | F         | 26-35     | 12        | A         | 4+        | 1         | 5         |  9        | 14        |  7092     | \n",
       "| 52073 | 1001942   | P00153742 | M         | 36-45     |  5        | B         | 1         | 1         | 8         |  9        | 14        | 10005     | \n",
       "| 469003 | 1000235   | P0094542  | M         | 26-35     |  0        | B         | 0         | 0         | 2         |  4        |  9        | 13074     | \n",
       "| 432762 | 1000647   | P00213342 | M         | 36-45     | 12        | B         | 1         | 1         | 5         |  9        | 14        |  7091     | \n",
       "| 65646 | 1004048   | P00220242 | F         | 36-45     |  1        | B         | 1         | 0         | 3         | 12        | 14        | 13251     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       User_ID Product_ID Gender Age   Occupation City_Category\n",
       "214016 1003056 P00034442  M      26-35 18         C            \n",
       "320723 1001396 P00281542  F      26-35 12         A            \n",
       "52073  1001942 P00153742  M      36-45  5         B            \n",
       "469003 1000235 P0094542   M      26-35  0         B            \n",
       "432762 1000647 P00213342  M      36-45 12         B            \n",
       "65646  1004048 P00220242  F      36-45  1         B            \n",
       "       Stay_In_Current_City_Years Marital_Status Product_Category_1\n",
       "214016 1                          1              1                 \n",
       "320723 4+                         1              5                 \n",
       "52073  1                          1              8                 \n",
       "469003 0                          0              2                 \n",
       "432762 1                          1              5                 \n",
       "65646  1                          0              3                 \n",
       "       Product_Category_2 Product_Category_3 Purchase\n",
       "214016 14                 16                 15345   \n",
       "320723  9                 14                  7092   \n",
       "52073   9                 14                 10005   \n",
       "469003  4                  9                 13074   \n",
       "432762  9                 14                  7091   \n",
       "65646  12                 14                 13251   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#set a random seed and shuffle\n",
    "set.seed(999)\n",
    "purchaseData = purchaseData[sample(1:nrow(purchaseData)), ]\n",
    "\n",
    "#Print first 6 rows of training data\n",
    "head(purchaseData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a shuffled dataset. From our summary analysis we know that the columns \"Age\" and \"Stay_In_Current_City_Years\" is of character type. But let's see if they have any levels of factors defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'0-17'</li>\n",
       "\t<li>'18-25'</li>\n",
       "\t<li>'26-35'</li>\n",
       "\t<li>'36-45'</li>\n",
       "\t<li>'46-50'</li>\n",
       "\t<li>'51-55'</li>\n",
       "\t<li>'55+'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '0-17'\n",
       "\\item '18-25'\n",
       "\\item '26-35'\n",
       "\\item '36-45'\n",
       "\\item '46-50'\n",
       "\\item '51-55'\n",
       "\\item '55+'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '0-17'\n",
       "2. '18-25'\n",
       "3. '26-35'\n",
       "4. '36-45'\n",
       "5. '46-50'\n",
       "6. '51-55'\n",
       "7. '55+'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"0-17\"  \"18-25\" \"26-35\" \"36-45\" \"46-50\" \"51-55\" \"55+\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Find the different age groups defined in the Age column\n",
    "Age_Groups = factor(purchaseData$Age)\n",
    "levels(Age_Groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 7 different age groups are defined in the column. But we can't have our data in this categorical format if we want to use XGBoost. Hence we'll use one-hot coding to create a sparse matrix for this categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot matrix for the column \"Age\"\n",
    "Age_Groups_Matrix = model.matrix(~Age-1, purchaseData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the same procedures of finding the factors and conversion procedure on Stay_In_Current_City_Years column as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'0'</li>\n",
       "\t<li>'1'</li>\n",
       "\t<li>'2'</li>\n",
       "\t<li>'3'</li>\n",
       "\t<li>'4+'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '0'\n",
       "\\item '1'\n",
       "\\item '2'\n",
       "\\item '3'\n",
       "\\item '4+'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '0'\n",
       "2. '1'\n",
       "3. '2'\n",
       "4. '3'\n",
       "5. '4+'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"0\"  \"1\"  \"2\"  \"3\"  \"4+\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Find the different values in the Stay_In_Current_City_Years column\n",
    "Years_Of_Stay = factor(purchaseData$Stay_In_Current_City_Years)\n",
    "levels(Years_Of_Stay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot matrix for the column \"Stay_In_Current_City_Years\"\n",
    "Years_Of_Stay_Matrix = model.matrix(~Stay_In_Current_City_Years-1, purchaseData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the only columns, besides \"Product_ID\", that are non-numeric are \"Gender\" and \"City_Category\" so we should encode them as well for XGBoost. We will not convert values of \"Product_ID\" because we won't be using this column for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot matrix for the column \"City_Category\"\n",
    "City_Category_Matrix = model.matrix(~City_Category-1, purchaseData)\n",
    "\n",
    "#One-hot matrix for the column \"Gender\"\n",
    "Gender_Matrix = model.matrix(~Gender-1, purchaseData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have made all necessary changes we must do the following:\n",
    "    1. Retain only numeric columns\n",
    "    2. Store the values of the target variable separately\n",
    "    3. Drop the target variable from the dataframe\n",
    "    4. Convert the final, all numeric dataframe into a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the subset of only numeric columns and \n",
    "# dropping the target variable \"Purchase\"\n",
    "purchaseData_Onlynumeric = subset(purchaseData, select = -c(Product_ID, Gender, Age, City_Category, Stay_In_Current_City_Years, Purchase))\n",
    "\n",
    "#Storing the target variable\n",
    "purchaseValues =  purchaseData$Purchase\n",
    "\n",
    "#Combining all the sparse matrices with the all numeric dataframe\n",
    "purchaseData_Matrix = cbind(purchaseData_Onlynumeric, Gender_Matrix, Age_Groups_Matrix, City_Category_Matrix, Years_Of_Stay_Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our complete matrix ready, let's split it into training and testing sets. The training set will have 80% of observations whereas the testing set will have 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_Observations = round(length(purchaseValues) * 0.7)\n",
    "\n",
    "#Training data and the corresponding target variables\n",
    "purchaseTrain = purchaseData_Matrix[1:n_Observations, ]\n",
    "purchaseValues_Train = purchaseValues[1:n_Observations]\n",
    "\n",
    "#Testing data and the corresponding target variables\n",
    "purchaseTest = purchaseData_Matrix[-(1:n_Observations), ]\n",
    "purchaseValues_Test =  purchaseValues[-(1:n_Observations)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further we convert the training and testing sets to dmatrix. This will quicken our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First convert the dataframe to a matrix\n",
    "purchaseTrain = as.matrix(purchaseTrain)\n",
    "purchaseTest = as.matrix(purchaseTest)\n",
    "\n",
    "#Then convert the matrix to a dmatrix\n",
    "d_train = xgb.DMatrix(data = purchaseTrain, label = purchaseValues_Train)\n",
    "d_test = xgb.DMatrix(data = purchaseTest, label = purchaseValues_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "Now that we have both the training and testing sets in the desired format, we shall proceed with training the initial model and measure the error to tune the model further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:7684.694336 \n",
      "[2]\ttrain-rmse:5787.736328 \n",
      "[3]\ttrain-rmse:4578.776367 \n",
      "[4]\ttrain-rmse:3847.539795 \n",
      "[5]\ttrain-rmse:3430.173096 \n",
      "[6]\ttrain-rmse:3206.232178 \n",
      "[7]\ttrain-rmse:3085.054688 \n",
      "[8]\ttrain-rmse:3021.175781 \n",
      "[9]\ttrain-rmse:2987.185547 \n",
      "[10]\ttrain-rmse:2967.730469 \n",
      "[11]\ttrain-rmse:2955.302246 \n",
      "[12]\ttrain-rmse:2946.292969 \n",
      "[13]\ttrain-rmse:2940.396729 \n",
      "[14]\ttrain-rmse:2935.267578 \n",
      "[15]\ttrain-rmse:2930.721924 \n",
      "[16]\ttrain-rmse:2925.091797 \n",
      "[17]\ttrain-rmse:2922.216797 \n",
      "[18]\ttrain-rmse:2917.476807 \n",
      "[19]\ttrain-rmse:2913.333252 \n",
      "[20]\ttrain-rmse:2911.729980 \n",
      "[21]\ttrain-rmse:2908.882568 \n",
      "[22]\ttrain-rmse:2903.523438 \n",
      "[23]\ttrain-rmse:2900.829102 \n",
      "[24]\ttrain-rmse:2898.748779 \n",
      "[25]\ttrain-rmse:2894.042480 \n",
      "[26]\ttrain-rmse:2889.906006 \n",
      "[27]\ttrain-rmse:2886.266357 \n",
      "[28]\ttrain-rmse:2885.307617 \n",
      "[29]\ttrain-rmse:2883.059570 \n",
      "[30]\ttrain-rmse:2880.306885 \n",
      "[31]\ttrain-rmse:2877.258301 \n",
      "[32]\ttrain-rmse:2876.264160 \n",
      "[33]\ttrain-rmse:2874.545898 \n",
      "[34]\ttrain-rmse:2870.686768 \n",
      "[35]\ttrain-rmse:2869.459717 \n",
      "[36]\ttrain-rmse:2868.697266 \n",
      "[37]\ttrain-rmse:2868.269287 \n",
      "[38]\ttrain-rmse:2865.809326 \n",
      "[39]\ttrain-rmse:2864.055664 \n",
      "[40]\ttrain-rmse:2861.119385 \n",
      "[41]\ttrain-rmse:2858.742676 \n",
      "[42]\ttrain-rmse:2857.658203 \n",
      "[43]\ttrain-rmse:2853.885498 \n",
      "[44]\ttrain-rmse:2850.701172 \n",
      "[45]\ttrain-rmse:2848.084473 \n",
      "[46]\ttrain-rmse:2846.832520 \n",
      "[47]\ttrain-rmse:2846.177246 \n",
      "[48]\ttrain-rmse:2844.592773 \n",
      "[49]\ttrain-rmse:2843.684326 \n",
      "[50]\ttrain-rmse:2841.322998 \n",
      "[51]\ttrain-rmse:2839.608643 \n",
      "[52]\ttrain-rmse:2837.000000 \n",
      "[53]\ttrain-rmse:2836.239258 \n",
      "[54]\ttrain-rmse:2833.600830 \n",
      "[55]\ttrain-rmse:2830.280762 \n",
      "[56]\ttrain-rmse:2829.219238 \n",
      "[57]\ttrain-rmse:2827.679443 \n",
      "[58]\ttrain-rmse:2824.947754 \n",
      "[59]\ttrain-rmse:2822.321533 \n",
      "[60]\ttrain-rmse:2820.849121 \n",
      "[61]\ttrain-rmse:2819.666260 \n",
      "[62]\ttrain-rmse:2817.713867 \n",
      "[63]\ttrain-rmse:2817.431396 \n",
      "[64]\ttrain-rmse:2816.857422 \n",
      "[65]\ttrain-rmse:2815.855469 \n",
      "[66]\ttrain-rmse:2813.243896 \n",
      "[67]\ttrain-rmse:2811.777344 \n",
      "[68]\ttrain-rmse:2811.031250 \n",
      "[69]\ttrain-rmse:2810.123535 \n",
      "[70]\ttrain-rmse:2809.615479 \n",
      "[71]\ttrain-rmse:2808.135498 \n",
      "[72]\ttrain-rmse:2806.377197 \n",
      "[73]\ttrain-rmse:2805.604980 \n",
      "[74]\ttrain-rmse:2804.646484 \n",
      "[75]\ttrain-rmse:2803.292236 \n",
      "[76]\ttrain-rmse:2802.176758 \n",
      "[77]\ttrain-rmse:2801.335693 \n",
      "[78]\ttrain-rmse:2800.695801 \n",
      "[79]\ttrain-rmse:2799.839355 \n",
      "[80]\ttrain-rmse:2799.133057 \n",
      "[81]\ttrain-rmse:2797.805420 \n",
      "[82]\ttrain-rmse:2796.068359 \n",
      "[83]\ttrain-rmse:2795.574951 \n",
      "[84]\ttrain-rmse:2793.342529 \n",
      "[85]\ttrain-rmse:2792.645264 \n",
      "[86]\ttrain-rmse:2791.818604 \n",
      "[87]\ttrain-rmse:2790.771240 \n",
      "[88]\ttrain-rmse:2788.713867 \n",
      "[89]\ttrain-rmse:2787.021729 \n",
      "[90]\ttrain-rmse:2786.322021 \n",
      "[91]\ttrain-rmse:2785.912354 \n",
      "[92]\ttrain-rmse:2784.284180 \n",
      "[93]\ttrain-rmse:2782.546875 \n",
      "[94]\ttrain-rmse:2780.765625 \n",
      "[95]\ttrain-rmse:2780.618896 \n",
      "[96]\ttrain-rmse:2779.632812 \n",
      "[97]\ttrain-rmse:2778.504883 \n",
      "[98]\ttrain-rmse:2777.007568 \n",
      "[99]\ttrain-rmse:2775.495361 \n",
      "[100]\ttrain-rmse:2774.491455 \n",
      "[101]\ttrain-rmse:2773.168701 \n",
      "[102]\ttrain-rmse:2772.520020 \n",
      "[103]\ttrain-rmse:2771.369141 \n",
      "[104]\ttrain-rmse:2770.663086 \n",
      "[105]\ttrain-rmse:2769.948486 \n",
      "[106]\ttrain-rmse:2769.542969 \n",
      "[107]\ttrain-rmse:2768.907227 \n",
      "[108]\ttrain-rmse:2767.491455 \n",
      "[109]\ttrain-rmse:2766.971191 \n",
      "[110]\ttrain-rmse:2766.592773 \n",
      "[111]\ttrain-rmse:2765.688232 \n",
      "[112]\ttrain-rmse:2764.330566 \n",
      "[113]\ttrain-rmse:2763.626465 \n",
      "[114]\ttrain-rmse:2762.915283 \n",
      "[115]\ttrain-rmse:2762.721924 \n",
      "[116]\ttrain-rmse:2762.282471 \n",
      "[117]\ttrain-rmse:2761.604736 \n",
      "[118]\ttrain-rmse:2760.387451 \n",
      "[119]\ttrain-rmse:2759.597412 \n",
      "[120]\ttrain-rmse:2758.101807 \n",
      "[121]\ttrain-rmse:2757.505127 \n",
      "[122]\ttrain-rmse:2756.570801 \n",
      "[123]\ttrain-rmse:2755.572266 \n",
      "[124]\ttrain-rmse:2754.691406 \n",
      "[125]\ttrain-rmse:2754.027100 \n",
      "[126]\ttrain-rmse:2752.880127 \n",
      "[127]\ttrain-rmse:2751.366211 \n",
      "[128]\ttrain-rmse:2750.533691 \n",
      "[129]\ttrain-rmse:2750.222900 \n",
      "[130]\ttrain-rmse:2749.184326 \n",
      "[131]\ttrain-rmse:2748.284180 \n",
      "[132]\ttrain-rmse:2747.195557 \n",
      "[133]\ttrain-rmse:2746.228516 \n",
      "[134]\ttrain-rmse:2745.375488 \n",
      "[135]\ttrain-rmse:2744.541016 \n",
      "[136]\ttrain-rmse:2743.695557 \n",
      "[137]\ttrain-rmse:2742.601318 \n",
      "[138]\ttrain-rmse:2741.988037 \n",
      "[139]\ttrain-rmse:2740.904785 \n",
      "[140]\ttrain-rmse:2740.305176 \n",
      "[141]\ttrain-rmse:2739.713379 \n",
      "[142]\ttrain-rmse:2738.884521 \n",
      "[143]\ttrain-rmse:2738.148193 \n",
      "[144]\ttrain-rmse:2737.233643 \n",
      "[145]\ttrain-rmse:2736.569824 \n",
      "[146]\ttrain-rmse:2735.558594 \n",
      "[147]\ttrain-rmse:2735.202881 \n",
      "[148]\ttrain-rmse:2734.708252 \n",
      "[149]\ttrain-rmse:2734.110596 \n",
      "[150]\ttrain-rmse:2733.672607 \n",
      "[151]\ttrain-rmse:2732.885254 \n",
      "[152]\ttrain-rmse:2732.292480 \n",
      "[153]\ttrain-rmse:2731.660156 \n",
      "[154]\ttrain-rmse:2731.116699 \n",
      "[155]\ttrain-rmse:2730.828369 \n",
      "[156]\ttrain-rmse:2730.170898 \n",
      "[157]\ttrain-rmse:2729.530273 \n",
      "[158]\ttrain-rmse:2728.814697 \n",
      "[159]\ttrain-rmse:2728.697754 \n",
      "[160]\ttrain-rmse:2727.852539 \n",
      "[161]\ttrain-rmse:2726.804932 \n",
      "[162]\ttrain-rmse:2726.255859 \n",
      "[163]\ttrain-rmse:2725.429443 \n",
      "[164]\ttrain-rmse:2724.375732 \n",
      "[165]\ttrain-rmse:2723.729492 \n",
      "[166]\ttrain-rmse:2723.291504 \n",
      "[167]\ttrain-rmse:2722.798340 \n",
      "[168]\ttrain-rmse:2721.758301 \n",
      "[169]\ttrain-rmse:2721.230225 \n",
      "[170]\ttrain-rmse:2720.712891 \n",
      "[171]\ttrain-rmse:2719.738525 \n",
      "[172]\ttrain-rmse:2719.304688 \n",
      "[173]\ttrain-rmse:2718.540527 \n",
      "[174]\ttrain-rmse:2717.551270 \n",
      "[175]\ttrain-rmse:2717.018799 \n",
      "[176]\ttrain-rmse:2716.412354 \n",
      "[177]\ttrain-rmse:2716.273682 \n",
      "[178]\ttrain-rmse:2715.533447 \n",
      "[179]\ttrain-rmse:2715.082031 \n",
      "[180]\ttrain-rmse:2714.375000 \n",
      "[181]\ttrain-rmse:2714.079102 \n",
      "[182]\ttrain-rmse:2713.464844 \n",
      "[183]\ttrain-rmse:2712.888672 \n",
      "[184]\ttrain-rmse:2712.360596 \n",
      "[185]\ttrain-rmse:2712.072510 \n",
      "[186]\ttrain-rmse:2711.657471 \n",
      "[187]\ttrain-rmse:2711.195068 \n",
      "[188]\ttrain-rmse:2710.837891 \n",
      "[189]\ttrain-rmse:2710.202148 \n",
      "[190]\ttrain-rmse:2709.378662 \n",
      "[191]\ttrain-rmse:2708.608398 \n",
      "[192]\ttrain-rmse:2708.111572 \n",
      "[193]\ttrain-rmse:2707.748535 \n",
      "[194]\ttrain-rmse:2707.143311 \n",
      "[195]\ttrain-rmse:2706.342041 \n",
      "[196]\ttrain-rmse:2706.082275 \n",
      "[197]\ttrain-rmse:2705.506348 \n",
      "[198]\ttrain-rmse:2704.970947 \n",
      "[199]\ttrain-rmse:2704.072754 \n",
      "[200]\ttrain-rmse:2703.093506 \n",
      "[201]\ttrain-rmse:2702.411865 \n",
      "[202]\ttrain-rmse:2701.926514 \n",
      "[203]\ttrain-rmse:2701.190674 \n",
      "[204]\ttrain-rmse:2700.351074 \n",
      "[205]\ttrain-rmse:2699.840332 \n",
      "[206]\ttrain-rmse:2699.292236 \n",
      "[207]\ttrain-rmse:2698.744873 \n",
      "[208]\ttrain-rmse:2698.089600 \n",
      "[209]\ttrain-rmse:2697.947754 \n",
      "[210]\ttrain-rmse:2697.665283 \n",
      "[211]\ttrain-rmse:2696.979980 \n",
      "[212]\ttrain-rmse:2696.717529 \n",
      "[213]\ttrain-rmse:2696.467529 \n",
      "[214]\ttrain-rmse:2695.832275 \n",
      "[215]\ttrain-rmse:2695.075439 \n",
      "[216]\ttrain-rmse:2694.570068 \n",
      "[217]\ttrain-rmse:2694.195068 \n",
      "[218]\ttrain-rmse:2693.811768 \n",
      "[219]\ttrain-rmse:2693.678223 \n",
      "[220]\ttrain-rmse:2693.319336 \n",
      "[221]\ttrain-rmse:2692.807373 \n",
      "[222]\ttrain-rmse:2692.604248 \n",
      "[223]\ttrain-rmse:2692.315918 \n",
      "[224]\ttrain-rmse:2691.920654 \n",
      "[225]\ttrain-rmse:2691.571045 \n",
      "[226]\ttrain-rmse:2690.916016 \n",
      "[227]\ttrain-rmse:2690.662109 \n",
      "[228]\ttrain-rmse:2690.427490 \n",
      "[229]\ttrain-rmse:2689.919922 \n",
      "[230]\ttrain-rmse:2689.534912 \n",
      "[231]\ttrain-rmse:2688.737305 \n",
      "[232]\ttrain-rmse:2688.362549 \n",
      "[233]\ttrain-rmse:2687.743408 \n",
      "[234]\ttrain-rmse:2687.321777 \n",
      "[235]\ttrain-rmse:2687.024170 \n",
      "[236]\ttrain-rmse:2686.491943 \n",
      "[237]\ttrain-rmse:2685.843750 \n",
      "[238]\ttrain-rmse:2685.245605 \n",
      "[239]\ttrain-rmse:2684.744385 \n",
      "[240]\ttrain-rmse:2684.074219 \n",
      "[241]\ttrain-rmse:2683.369141 \n",
      "[242]\ttrain-rmse:2682.800293 \n",
      "[243]\ttrain-rmse:2682.496094 \n",
      "[244]\ttrain-rmse:2682.197021 \n",
      "[245]\ttrain-rmse:2681.763184 \n",
      "[246]\ttrain-rmse:2681.209229 \n",
      "[247]\ttrain-rmse:2680.788330 \n",
      "[248]\ttrain-rmse:2680.425781 \n",
      "[249]\ttrain-rmse:2679.804199 \n",
      "[250]\ttrain-rmse:2679.164795 \n",
      "[251]\ttrain-rmse:2678.725098 \n",
      "[252]\ttrain-rmse:2678.458252 \n",
      "[253]\ttrain-rmse:2677.822021 \n",
      "[254]\ttrain-rmse:2677.407715 \n",
      "[255]\ttrain-rmse:2677.232666 \n",
      "[256]\ttrain-rmse:2677.070068 \n",
      "[257]\ttrain-rmse:2676.762695 \n",
      "[258]\ttrain-rmse:2675.841064 \n",
      "[259]\ttrain-rmse:2675.384766 \n",
      "[260]\ttrain-rmse:2674.922852 \n",
      "[261]\ttrain-rmse:2674.468262 \n",
      "[262]\ttrain-rmse:2674.281494 \n",
      "[263]\ttrain-rmse:2673.615479 \n",
      "[264]\ttrain-rmse:2673.239014 \n",
      "[265]\ttrain-rmse:2672.753418 \n",
      "[266]\ttrain-rmse:2672.368896 \n",
      "[267]\ttrain-rmse:2671.768311 \n",
      "[268]\ttrain-rmse:2671.469238 \n",
      "[269]\ttrain-rmse:2670.795410 \n",
      "[270]\ttrain-rmse:2670.266113 \n",
      "[271]\ttrain-rmse:2670.005127 \n",
      "[272]\ttrain-rmse:2669.152832 \n",
      "[273]\ttrain-rmse:2668.804688 \n",
      "[274]\ttrain-rmse:2668.361572 \n",
      "[275]\ttrain-rmse:2667.927734 \n",
      "[276]\ttrain-rmse:2667.703613 \n",
      "[277]\ttrain-rmse:2667.356445 \n",
      "[278]\ttrain-rmse:2667.164795 \n",
      "[279]\ttrain-rmse:2666.437988 \n",
      "[280]\ttrain-rmse:2666.064697 \n",
      "[281]\ttrain-rmse:2665.647949 \n",
      "[282]\ttrain-rmse:2665.069336 \n",
      "[283]\ttrain-rmse:2664.720215 \n",
      "[284]\ttrain-rmse:2664.498779 \n",
      "[285]\ttrain-rmse:2664.041260 \n",
      "[286]\ttrain-rmse:2663.456787 \n",
      "[287]\ttrain-rmse:2662.933350 \n",
      "[288]\ttrain-rmse:2662.546875 \n",
      "[289]\ttrain-rmse:2662.275879 \n",
      "[290]\ttrain-rmse:2661.707520 \n",
      "[291]\ttrain-rmse:2661.292480 \n",
      "[292]\ttrain-rmse:2660.822266 \n",
      "[293]\ttrain-rmse:2660.433838 \n",
      "[294]\ttrain-rmse:2659.838623 \n",
      "[295]\ttrain-rmse:2659.465820 \n",
      "[296]\ttrain-rmse:2659.210693 \n",
      "[297]\ttrain-rmse:2659.118408 \n",
      "[298]\ttrain-rmse:2658.687256 \n",
      "[299]\ttrain-rmse:2658.133789 \n",
      "[300]\ttrain-rmse:2657.688477 \n",
      "[301]\ttrain-rmse:2657.304932 \n",
      "[302]\ttrain-rmse:2656.876221 \n",
      "[303]\ttrain-rmse:2656.453857 \n",
      "[304]\ttrain-rmse:2655.957031 \n",
      "[305]\ttrain-rmse:2655.576660 \n",
      "[306]\ttrain-rmse:2655.391113 \n",
      "[307]\ttrain-rmse:2655.075684 \n",
      "[308]\ttrain-rmse:2654.777588 \n",
      "[309]\ttrain-rmse:2654.201416 \n",
      "[310]\ttrain-rmse:2653.899414 \n",
      "[311]\ttrain-rmse:2653.603027 \n",
      "[312]\ttrain-rmse:2653.317139 \n",
      "[313]\ttrain-rmse:2652.561768 \n",
      "[314]\ttrain-rmse:2652.209229 \n",
      "[315]\ttrain-rmse:2651.913086 \n",
      "[316]\ttrain-rmse:2651.669189 \n",
      "[317]\ttrain-rmse:2650.885742 \n",
      "[318]\ttrain-rmse:2650.627197 \n",
      "[319]\ttrain-rmse:2650.231445 \n",
      "[320]\ttrain-rmse:2649.925049 \n",
      "[321]\ttrain-rmse:2649.731201 \n",
      "[322]\ttrain-rmse:2649.262695 \n",
      "[323]\ttrain-rmse:2648.606689 \n",
      "[324]\ttrain-rmse:2648.328125 \n",
      "[325]\ttrain-rmse:2647.847412 \n",
      "[326]\ttrain-rmse:2647.506104 \n",
      "[327]\ttrain-rmse:2647.165771 \n",
      "[328]\ttrain-rmse:2646.776123 \n",
      "[329]\ttrain-rmse:2646.688477 \n",
      "[330]\ttrain-rmse:2646.232910 \n",
      "[331]\ttrain-rmse:2645.956543 \n",
      "[332]\ttrain-rmse:2645.593018 \n",
      "[333]\ttrain-rmse:2645.101074 \n",
      "[334]\ttrain-rmse:2644.871582 \n",
      "[335]\ttrain-rmse:2644.611816 \n",
      "[336]\ttrain-rmse:2644.492188 \n",
      "[337]\ttrain-rmse:2644.167969 \n",
      "[338]\ttrain-rmse:2643.958496 \n",
      "[339]\ttrain-rmse:2643.585693 \n",
      "[340]\ttrain-rmse:2643.317627 \n",
      "[341]\ttrain-rmse:2643.126709 \n",
      "[342]\ttrain-rmse:2642.819824 \n",
      "[343]\ttrain-rmse:2642.367676 \n",
      "[344]\ttrain-rmse:2642.088379 \n",
      "[345]\ttrain-rmse:2641.846436 \n",
      "[346]\ttrain-rmse:2641.592529 \n",
      "[347]\ttrain-rmse:2641.297119 \n",
      "[348]\ttrain-rmse:2641.120117 \n",
      "[349]\ttrain-rmse:2640.849121 \n",
      "[350]\ttrain-rmse:2640.557373 \n",
      "[351]\ttrain-rmse:2640.231445 \n",
      "[352]\ttrain-rmse:2639.921143 \n",
      "[353]\ttrain-rmse:2639.462891 \n",
      "[354]\ttrain-rmse:2638.885742 \n",
      "[355]\ttrain-rmse:2638.402832 \n",
      "[356]\ttrain-rmse:2637.998779 \n",
      "[357]\ttrain-rmse:2637.529785 \n",
      "[358]\ttrain-rmse:2637.333496 \n",
      "[359]\ttrain-rmse:2636.805176 \n",
      "[360]\ttrain-rmse:2636.502441 \n",
      "[361]\ttrain-rmse:2636.104248 \n",
      "[362]\ttrain-rmse:2635.877197 \n",
      "[363]\ttrain-rmse:2635.639404 \n",
      "[364]\ttrain-rmse:2635.482178 \n",
      "[365]\ttrain-rmse:2635.195068 \n",
      "[366]\ttrain-rmse:2634.675049 \n",
      "[367]\ttrain-rmse:2634.297119 \n",
      "[368]\ttrain-rmse:2634.142090 \n",
      "[369]\ttrain-rmse:2633.731934 \n",
      "[370]\ttrain-rmse:2633.346924 \n",
      "[371]\ttrain-rmse:2633.065918 \n",
      "[372]\ttrain-rmse:2632.494385 \n",
      "[373]\ttrain-rmse:2631.805908 \n",
      "[374]\ttrain-rmse:2631.393066 \n",
      "[375]\ttrain-rmse:2631.044922 \n",
      "[376]\ttrain-rmse:2630.633301 \n",
      "[377]\ttrain-rmse:2629.986572 \n",
      "[378]\ttrain-rmse:2629.458740 \n",
      "[379]\ttrain-rmse:2628.996094 \n",
      "[380]\ttrain-rmse:2628.540283 \n",
      "[381]\ttrain-rmse:2628.287842 \n",
      "[382]\ttrain-rmse:2627.962402 \n",
      "[383]\ttrain-rmse:2627.649902 \n",
      "[384]\ttrain-rmse:2627.298584 \n",
      "[385]\ttrain-rmse:2626.956299 \n",
      "[386]\ttrain-rmse:2626.668213 \n",
      "[387]\ttrain-rmse:2626.489990 \n",
      "[388]\ttrain-rmse:2626.141602 \n",
      "[389]\ttrain-rmse:2625.914551 \n",
      "[390]\ttrain-rmse:2625.464844 \n",
      "[391]\ttrain-rmse:2625.056152 \n",
      "[392]\ttrain-rmse:2624.750000 \n",
      "[393]\ttrain-rmse:2624.402100 \n",
      "[394]\ttrain-rmse:2624.095459 \n",
      "[395]\ttrain-rmse:2623.946533 \n",
      "[396]\ttrain-rmse:2623.554199 \n",
      "[397]\ttrain-rmse:2623.283203 \n",
      "[398]\ttrain-rmse:2622.696289 \n",
      "[399]\ttrain-rmse:2622.461426 \n",
      "[400]\ttrain-rmse:2622.025879 \n",
      "[401]\ttrain-rmse:2621.855225 \n",
      "[402]\ttrain-rmse:2621.655762 \n",
      "[403]\ttrain-rmse:2621.487549 \n",
      "[404]\ttrain-rmse:2621.200928 \n",
      "[405]\ttrain-rmse:2621.041992 \n",
      "[406]\ttrain-rmse:2620.828613 \n",
      "[407]\ttrain-rmse:2620.387939 \n",
      "[408]\ttrain-rmse:2620.076660 \n",
      "[409]\ttrain-rmse:2619.687012 \n",
      "[410]\ttrain-rmse:2619.200439 \n",
      "[411]\ttrain-rmse:2618.988525 \n",
      "[412]\ttrain-rmse:2618.496826 \n",
      "[413]\ttrain-rmse:2618.117920 \n",
      "[414]\ttrain-rmse:2617.698730 \n",
      "[415]\ttrain-rmse:2617.407471 \n",
      "[416]\ttrain-rmse:2617.288086 \n",
      "[417]\ttrain-rmse:2616.879150 \n",
      "[418]\ttrain-rmse:2616.777832 \n",
      "[419]\ttrain-rmse:2616.357666 \n",
      "[420]\ttrain-rmse:2616.034668 \n",
      "[421]\ttrain-rmse:2615.673828 \n",
      "[422]\ttrain-rmse:2615.240234 \n",
      "[423]\ttrain-rmse:2614.871338 \n",
      "[424]\ttrain-rmse:2614.700195 \n",
      "[425]\ttrain-rmse:2614.402588 \n",
      "[426]\ttrain-rmse:2613.965332 \n",
      "[427]\ttrain-rmse:2613.479980 \n",
      "[428]\ttrain-rmse:2613.249512 \n",
      "[429]\ttrain-rmse:2612.952637 \n",
      "[430]\ttrain-rmse:2612.442627 \n",
      "[431]\ttrain-rmse:2612.115234 \n",
      "[432]\ttrain-rmse:2611.679932 \n",
      "[433]\ttrain-rmse:2611.309814 \n",
      "[434]\ttrain-rmse:2610.901367 \n",
      "[435]\ttrain-rmse:2610.552246 \n",
      "[436]\ttrain-rmse:2610.378174 \n",
      "[437]\ttrain-rmse:2609.893555 \n",
      "[438]\ttrain-rmse:2609.545898 \n",
      "[439]\ttrain-rmse:2609.247070 \n",
      "[440]\ttrain-rmse:2608.907471 \n",
      "[441]\ttrain-rmse:2608.485840 \n",
      "[442]\ttrain-rmse:2608.190674 \n",
      "[443]\ttrain-rmse:2607.696533 \n",
      "[444]\ttrain-rmse:2607.354980 \n",
      "[445]\ttrain-rmse:2607.276367 \n",
      "[446]\ttrain-rmse:2607.163330 \n",
      "[447]\ttrain-rmse:2607.053711 \n",
      "[448]\ttrain-rmse:2606.726562 \n",
      "[449]\ttrain-rmse:2606.494629 \n",
      "[450]\ttrain-rmse:2606.218262 \n",
      "[451]\ttrain-rmse:2605.728271 \n",
      "[452]\ttrain-rmse:2605.427734 \n",
      "[453]\ttrain-rmse:2605.332031 \n",
      "[454]\ttrain-rmse:2605.258545 \n",
      "[455]\ttrain-rmse:2604.974609 \n",
      "[456]\ttrain-rmse:2604.736084 \n",
      "[457]\ttrain-rmse:2604.344971 \n",
      "[458]\ttrain-rmse:2604.087891 \n",
      "[459]\ttrain-rmse:2603.767578 \n",
      "[460]\ttrain-rmse:2603.444824 \n",
      "[461]\ttrain-rmse:2603.106445 \n",
      "[462]\ttrain-rmse:2602.709717 \n",
      "[463]\ttrain-rmse:2602.541016 \n",
      "[464]\ttrain-rmse:2602.484619 \n",
      "[465]\ttrain-rmse:2602.177490 \n",
      "[466]\ttrain-rmse:2601.802734 \n",
      "[467]\ttrain-rmse:2601.589844 \n",
      "[468]\ttrain-rmse:2601.239502 \n",
      "[469]\ttrain-rmse:2601.152832 \n",
      "[470]\ttrain-rmse:2600.841064 \n",
      "[471]\ttrain-rmse:2600.577393 \n",
      "[472]\ttrain-rmse:2600.305908 \n",
      "[473]\ttrain-rmse:2600.197510 \n",
      "[474]\ttrain-rmse:2599.876465 \n",
      "[475]\ttrain-rmse:2599.542969 \n",
      "[476]\ttrain-rmse:2599.228271 \n",
      "[477]\ttrain-rmse:2599.031250 \n",
      "[478]\ttrain-rmse:2598.627441 \n",
      "[479]\ttrain-rmse:2598.253418 \n",
      "[480]\ttrain-rmse:2597.799316 \n",
      "[481]\ttrain-rmse:2597.390137 \n",
      "[482]\ttrain-rmse:2597.141602 \n",
      "[483]\ttrain-rmse:2596.847900 \n",
      "[484]\ttrain-rmse:2596.683350 \n",
      "[485]\ttrain-rmse:2596.382080 \n",
      "[486]\ttrain-rmse:2596.156494 \n",
      "[487]\ttrain-rmse:2595.812500 \n",
      "[488]\ttrain-rmse:2595.556885 \n",
      "[489]\ttrain-rmse:2595.364502 \n",
      "[490]\ttrain-rmse:2594.940674 \n",
      "[491]\ttrain-rmse:2594.639893 \n",
      "[492]\ttrain-rmse:2594.283691 \n",
      "[493]\ttrain-rmse:2593.964355 \n",
      "[494]\ttrain-rmse:2593.657959 \n",
      "[495]\ttrain-rmse:2593.300781 \n",
      "[496]\ttrain-rmse:2593.069336 \n",
      "[497]\ttrain-rmse:2592.727295 \n",
      "[498]\ttrain-rmse:2592.415527 \n",
      "[499]\ttrain-rmse:2592.043457 \n",
      "[500]\ttrain-rmse:2591.739990 \n"
     ]
    }
   ],
   "source": [
    "model = xgboost(data = d_train, nround = 500, objective = \"reg:linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the model we just trained, we see that the Root Mean Square Error (RMSE) decreases rapidly, coming as low as 2591 from the initial RMSE value of more than 7000. Now let's see how this model performs with our test data. We'll take the average of the error as observed in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2758.96513161713"
      ],
      "text/latex": [
       "2758.96513161713"
      ],
      "text/markdown": [
       "2758.96513161713"
      ],
      "text/plain": [
       "[1] 2758.965"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Calculate the purchase price for testing data\n",
    "pred_purchase = predict(model, d_test)\n",
    "\n",
    "#Calculate the RMSE observed\n",
    "purchase_err = pred_purchase - purchaseValues_Test\n",
    "rmse = sqrt(mean(purchase_err^2))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there's a significant difference in the RMSE values calculated for the training and the testing set. The model, although performing fairly well, is overfitting a little bit. We can reduce this error by further tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the model\n",
    "\n",
    "To reduce overfitting we will tune the model in the following aspects:\n",
    "    1. Control complexity: this means including max_depth, min_child_weight, and gamma\n",
    "    2. Introduce randomness: this means including colsample_bytree, subsample, and eta\n",
    "    \n",
    "So now we must tune and create different models with varied parameter values and then finalize the model that produces the least RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:9729.503906 \n",
      "[2]\ttrain-rmse:8963.846680 \n",
      "[3]\ttrain-rmse:8313.514648 \n",
      "[4]\ttrain-rmse:7595.486328 \n",
      "[5]\ttrain-rmse:7070.075684 \n",
      "[6]\ttrain-rmse:6637.600586 \n",
      "[7]\ttrain-rmse:6271.372070 \n",
      "[8]\ttrain-rmse:5788.150879 \n",
      "[9]\ttrain-rmse:5500.750488 \n",
      "[10]\ttrain-rmse:5107.929199 \n",
      "[11]\ttrain-rmse:4899.496094 \n",
      "[12]\ttrain-rmse:4724.195801 \n",
      "[13]\ttrain-rmse:4557.495605 \n",
      "[14]\ttrain-rmse:4439.103516 \n",
      "[15]\ttrain-rmse:4329.604980 \n",
      "[16]\ttrain-rmse:4227.562988 \n",
      "[17]\ttrain-rmse:4008.449463 \n",
      "[18]\ttrain-rmse:3937.497559 \n",
      "[19]\ttrain-rmse:3881.888672 \n",
      "[20]\ttrain-rmse:3835.854248 \n",
      "[21]\ttrain-rmse:3792.937744 \n",
      "[22]\ttrain-rmse:3755.088623 \n",
      "[23]\ttrain-rmse:3604.050293 \n",
      "[24]\ttrain-rmse:3580.314453 \n",
      "[25]\ttrain-rmse:3456.925293 \n",
      "[26]\ttrain-rmse:3439.077393 \n",
      "[27]\ttrain-rmse:3425.802490 \n",
      "[28]\ttrain-rmse:3326.429199 \n",
      "[29]\ttrain-rmse:3249.381836 \n",
      "[30]\ttrain-rmse:3180.477051 \n",
      "[31]\ttrain-rmse:3166.950684 \n",
      "[32]\ttrain-rmse:3105.676758 \n",
      "[33]\ttrain-rmse:3056.200684 \n",
      "[34]\ttrain-rmse:3050.219238 \n",
      "[35]\ttrain-rmse:3004.955811 \n",
      "[36]\ttrain-rmse:3000.045410 \n",
      "[37]\ttrain-rmse:2993.696777 \n",
      "[38]\ttrain-rmse:2956.819336 \n",
      "[39]\ttrain-rmse:2952.658203 \n",
      "[40]\ttrain-rmse:2949.137939 \n",
      "[41]\ttrain-rmse:2924.840576 \n",
      "[42]\ttrain-rmse:2922.138184 \n",
      "[43]\ttrain-rmse:2920.807129 \n",
      "[44]\ttrain-rmse:2918.032471 \n",
      "[45]\ttrain-rmse:2916.783203 \n",
      "[46]\ttrain-rmse:2913.926270 \n",
      "[47]\ttrain-rmse:2890.036133 \n",
      "[48]\ttrain-rmse:2871.247559 \n",
      "[49]\ttrain-rmse:2869.845947 \n",
      "[50]\ttrain-rmse:2868.580811 \n"
     ]
    }
   ],
   "source": [
    "model_tuned1 = xgboost(data = d_train,\n",
    "                      eta = 0.1,                     #controls the step size shrinkage, default=0.3\n",
    "                      nround = 50,                   #since we are decreasing the eta, we need high nround value\n",
    "                      objective = \"reg:linear\",\n",
    "                      max_depth = 15,                #reduces the depth of the trees to reduce complexity\n",
    "                      min_child_weight = 100,        #higher the value, more instances required to make a node\n",
    "                      gamma = 100,                   #minimum loss reduction required to make further partition of a leaf node\n",
    "                      subsample = 0.5,               #subsamples the instances in the training set by half\n",
    "                      colsample_bytree = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"RMSE1:  2911.17034740029\"\n",
      "[1] \"RMSE1 difference:  42.5895364002936\"\n"
     ]
    }
   ],
   "source": [
    "#Calculate the purchase price for testing data\n",
    "pred_purchase1 = predict(model_tuned1, d_test)\n",
    "\n",
    "#Calculate the RMSE observed\n",
    "purchase_err1 = pred_purchase1 - purchaseValues_Test\n",
    "rmse1 = sqrt(mean(purchase_err1^2))\n",
    "print(paste(\"RMSE1: \",rmse1))\n",
    "\n",
    "#Fetching the RMSE calculated in the last iteration\n",
    "eval_log = model_tuned1[4]\n",
    "log = eval_log[[1]]\n",
    "rmse_calc1 = log$train_rmse[log$iter==50]\n",
    "print(paste(\"RMSE1 difference: \", rmse1-rmse_calc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:9578.111328 \n",
      "[2]\ttrain-rmse:8810.934570 \n",
      "[3]\ttrain-rmse:8147.248535 \n",
      "[4]\ttrain-rmse:7447.132812 \n",
      "[5]\ttrain-rmse:6938.682129 \n",
      "[6]\ttrain-rmse:6376.820312 \n",
      "[7]\ttrain-rmse:5971.988770 \n",
      "[8]\ttrain-rmse:5649.116211 \n",
      "[9]\ttrain-rmse:5245.663574 \n",
      "[10]\ttrain-rmse:4895.776367 \n",
      "[11]\ttrain-rmse:4590.961426 \n",
      "[12]\ttrain-rmse:4327.969727 \n",
      "[13]\ttrain-rmse:4163.589844 \n",
      "[14]\ttrain-rmse:3961.343994 \n",
      "[15]\ttrain-rmse:3838.519287 \n",
      "[16]\ttrain-rmse:3740.436035 \n",
      "[17]\ttrain-rmse:3603.364502 \n",
      "[18]\ttrain-rmse:3483.074463 \n",
      "[19]\ttrain-rmse:3384.973633 \n",
      "[20]\ttrain-rmse:3304.579590 \n",
      "[21]\ttrain-rmse:3265.326416 \n",
      "[22]\ttrain-rmse:3203.305420 \n",
      "[23]\ttrain-rmse:3173.202148 \n",
      "[24]\ttrain-rmse:3127.561279 \n",
      "[25]\ttrain-rmse:3088.990234 \n",
      "[26]\ttrain-rmse:3057.566406 \n",
      "[27]\ttrain-rmse:3044.606934 \n",
      "[28]\ttrain-rmse:3020.217285 \n",
      "[29]\ttrain-rmse:2999.631836 \n",
      "[30]\ttrain-rmse:2980.692627 \n",
      "[31]\ttrain-rmse:2966.509766 \n",
      "[32]\ttrain-rmse:2959.669678 \n",
      "[33]\ttrain-rmse:2945.194092 \n",
      "[34]\ttrain-rmse:2941.806641 \n",
      "[35]\ttrain-rmse:2935.823975 \n",
      "[36]\ttrain-rmse:2924.900146 \n",
      "[37]\ttrain-rmse:2921.930176 \n",
      "[38]\ttrain-rmse:2919.967041 \n",
      "[39]\ttrain-rmse:2912.393555 \n",
      "[40]\ttrain-rmse:2910.275391 \n",
      "[41]\ttrain-rmse:2907.735596 \n",
      "[42]\ttrain-rmse:2901.420654 \n",
      "[43]\ttrain-rmse:2899.984375 \n",
      "[44]\ttrain-rmse:2895.232422 \n",
      "[45]\ttrain-rmse:2891.769775 \n",
      "[46]\ttrain-rmse:2888.857910 \n",
      "[47]\ttrain-rmse:2888.275391 \n",
      "[48]\ttrain-rmse:2887.002197 \n",
      "[49]\ttrain-rmse:2885.625732 \n",
      "[50]\ttrain-rmse:2884.002197 \n",
      "[51]\ttrain-rmse:2880.373535 \n",
      "[52]\ttrain-rmse:2878.177002 \n",
      "[53]\ttrain-rmse:2875.369873 \n",
      "[54]\ttrain-rmse:2871.893799 \n",
      "[55]\ttrain-rmse:2869.760010 \n",
      "[56]\ttrain-rmse:2867.938477 \n",
      "[57]\ttrain-rmse:2865.508545 \n",
      "[58]\ttrain-rmse:2863.927490 \n",
      "[59]\ttrain-rmse:2861.178955 \n",
      "[60]\ttrain-rmse:2859.575684 \n",
      "[61]\ttrain-rmse:2856.927246 \n",
      "[62]\ttrain-rmse:2855.809082 \n",
      "[63]\ttrain-rmse:2855.630127 \n",
      "[64]\ttrain-rmse:2854.444824 \n",
      "[65]\ttrain-rmse:2851.709473 \n",
      "[66]\ttrain-rmse:2851.278809 \n",
      "[67]\ttrain-rmse:2850.406494 \n",
      "[68]\ttrain-rmse:2848.856445 \n",
      "[69]\ttrain-rmse:2847.563477 \n",
      "[70]\ttrain-rmse:2845.940430 \n",
      "[71]\ttrain-rmse:2844.788330 \n",
      "[72]\ttrain-rmse:2843.551270 \n",
      "[73]\ttrain-rmse:2842.024658 \n",
      "[74]\ttrain-rmse:2840.718506 \n",
      "[75]\ttrain-rmse:2839.464111 \n",
      "[76]\ttrain-rmse:2838.523438 \n",
      "[77]\ttrain-rmse:2837.032227 \n",
      "[78]\ttrain-rmse:2836.685303 \n",
      "[79]\ttrain-rmse:2835.781006 \n",
      "[80]\ttrain-rmse:2835.604004 \n",
      "[81]\ttrain-rmse:2835.164795 \n",
      "[82]\ttrain-rmse:2833.591309 \n",
      "[83]\ttrain-rmse:2832.828857 \n",
      "[84]\ttrain-rmse:2832.004883 \n",
      "[85]\ttrain-rmse:2830.967529 \n",
      "[86]\ttrain-rmse:2830.590088 \n",
      "[87]\ttrain-rmse:2830.405273 \n",
      "[88]\ttrain-rmse:2829.020264 \n",
      "[89]\ttrain-rmse:2827.599121 \n",
      "[90]\ttrain-rmse:2827.262695 \n",
      "[91]\ttrain-rmse:2826.179199 \n",
      "[92]\ttrain-rmse:2825.077637 \n",
      "[93]\ttrain-rmse:2824.416992 \n",
      "[94]\ttrain-rmse:2824.084717 \n",
      "[95]\ttrain-rmse:2823.379639 \n",
      "[96]\ttrain-rmse:2822.773438 \n",
      "[97]\ttrain-rmse:2822.517334 \n",
      "[98]\ttrain-rmse:2821.100830 \n",
      "[99]\ttrain-rmse:2819.535156 \n",
      "[100]\ttrain-rmse:2817.468018 \n"
     ]
    }
   ],
   "source": [
    "model_tuned2 = xgboost(data = d_train,\n",
    "                      eta = 0.1,                     #controls the step size shrinkage, default=0.3\n",
    "                      nround = 100,                  #since we are decreasing the eta, we need high nround value\n",
    "                      objective = \"reg:linear\",\n",
    "                      max_depth = 10,                #reduces the depth of the trees to reduce complexity\n",
    "                      min_child_weight = 150,        #higher the value, more instances required to make a node\n",
    "                      gamma = 100,                   #minimum loss reduction required to make further partition of a leaf node\n",
    "                      subsample = 0.5,               #subsamples the instances in the training set by half\n",
    "                      colsample_bytree = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"RMSE2:  2839.06907897793\"\n",
      "[1] \"RMSE2 difference:  21.6010609779319\"\n"
     ]
    }
   ],
   "source": [
    "#Calculate the purchase price for testing data\n",
    "pred_purchase2 = predict(model_tuned2, d_test)\n",
    "\n",
    "#Calculate the RMSE observed\n",
    "purchase_err2 = pred_purchase2 - purchaseValues_Test\n",
    "rmse2 = sqrt(mean(purchase_err2^2))\n",
    "print(paste(\"RMSE2: \",rmse2))\n",
    "\n",
    "#Fetching the RMSE calculated in the last iteration\n",
    "eval_log = model_tuned2[4]\n",
    "log = eval_log[[1]]\n",
    "rmse_calc2 = log$train_rmse[log$iter==100]\n",
    "print(paste(\"RMSE2 difference: \", rmse2-rmse_calc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:9579.466797 \n",
      "[2]\ttrain-rmse:8811.900391 \n",
      "[3]\ttrain-rmse:8160.341797 \n",
      "[4]\ttrain-rmse:7566.859375 \n",
      "[5]\ttrain-rmse:6931.979492 \n",
      "[6]\ttrain-rmse:6370.618164 \n",
      "[7]\ttrain-rmse:5991.699707 \n",
      "[8]\ttrain-rmse:5545.004883 \n",
      "[9]\ttrain-rmse:5153.651367 \n",
      "[10]\ttrain-rmse:4899.008789 \n",
      "[11]\ttrain-rmse:4594.241699 \n",
      "[12]\ttrain-rmse:4329.546387 \n",
      "[13]\ttrain-rmse:4104.196777 \n",
      "[14]\ttrain-rmse:3911.833740 \n",
      "[15]\ttrain-rmse:3746.194580 \n",
      "[16]\ttrain-rmse:3607.281006 \n",
      "[17]\ttrain-rmse:3488.724365 \n",
      "[18]\ttrain-rmse:3419.525635 \n",
      "[19]\ttrain-rmse:3360.657471 \n",
      "[20]\ttrain-rmse:3313.066406 \n",
      "[21]\ttrain-rmse:3243.010742 \n",
      "[22]\ttrain-rmse:3183.727051 \n",
      "[23]\ttrain-rmse:3134.784668 \n",
      "[24]\ttrain-rmse:3092.535156 \n",
      "[25]\ttrain-rmse:3059.231934 \n",
      "[26]\ttrain-rmse:3032.929199 \n",
      "[27]\ttrain-rmse:3019.089600 \n",
      "[28]\ttrain-rmse:3007.075684 \n",
      "[29]\ttrain-rmse:2997.207520 \n",
      "[30]\ttrain-rmse:2989.613281 \n",
      "[31]\ttrain-rmse:2984.018799 \n",
      "[32]\ttrain-rmse:2968.790283 \n",
      "[33]\ttrain-rmse:2954.701172 \n",
      "[34]\ttrain-rmse:2944.305664 \n",
      "[35]\ttrain-rmse:2940.844971 \n",
      "[36]\ttrain-rmse:2932.080322 \n",
      "[37]\ttrain-rmse:2929.562988 \n",
      "[38]\ttrain-rmse:2926.038818 \n",
      "[39]\ttrain-rmse:2924.310303 \n",
      "[40]\ttrain-rmse:2918.970459 \n",
      "[41]\ttrain-rmse:2917.735107 \n",
      "[42]\ttrain-rmse:2915.650635 \n",
      "[43]\ttrain-rmse:2913.991211 \n",
      "[44]\ttrain-rmse:2909.152588 \n",
      "[45]\ttrain-rmse:2903.591797 \n",
      "[46]\ttrain-rmse:2901.885742 \n",
      "[47]\ttrain-rmse:2898.646484 \n",
      "[48]\ttrain-rmse:2894.583496 \n",
      "[49]\ttrain-rmse:2891.805176 \n",
      "[50]\ttrain-rmse:2888.833984 \n",
      "[51]\ttrain-rmse:2887.990723 \n",
      "[52]\ttrain-rmse:2884.959961 \n",
      "[53]\ttrain-rmse:2884.567871 \n",
      "[54]\ttrain-rmse:2884.151611 \n",
      "[55]\ttrain-rmse:2882.443359 \n",
      "[56]\ttrain-rmse:2879.968506 \n",
      "[57]\ttrain-rmse:2877.981934 \n",
      "[58]\ttrain-rmse:2876.709229 \n",
      "[59]\ttrain-rmse:2875.578857 \n",
      "[60]\ttrain-rmse:2873.607910 \n",
      "[61]\ttrain-rmse:2873.286377 \n",
      "[62]\ttrain-rmse:2872.772461 \n",
      "[63]\ttrain-rmse:2871.369873 \n",
      "[64]\ttrain-rmse:2870.483398 \n",
      "[65]\ttrain-rmse:2869.167480 \n",
      "[66]\ttrain-rmse:2867.561523 \n",
      "[67]\ttrain-rmse:2866.803223 \n",
      "[68]\ttrain-rmse:2866.327881 \n",
      "[69]\ttrain-rmse:2865.945068 \n",
      "[70]\ttrain-rmse:2865.239502 \n",
      "[71]\ttrain-rmse:2863.516113 \n",
      "[72]\ttrain-rmse:2862.476318 \n",
      "[73]\ttrain-rmse:2860.678711 \n",
      "[74]\ttrain-rmse:2859.254395 \n",
      "[75]\ttrain-rmse:2858.852295 \n",
      "[76]\ttrain-rmse:2858.189453 \n",
      "[77]\ttrain-rmse:2857.520752 \n",
      "[78]\ttrain-rmse:2855.784668 \n",
      "[79]\ttrain-rmse:2855.502197 \n",
      "[80]\ttrain-rmse:2855.137207 \n",
      "[81]\ttrain-rmse:2853.641357 \n",
      "[82]\ttrain-rmse:2852.552490 \n",
      "[83]\ttrain-rmse:2852.130127 \n",
      "[84]\ttrain-rmse:2850.988281 \n",
      "[85]\ttrain-rmse:2850.509277 \n",
      "[86]\ttrain-rmse:2849.389404 \n",
      "[87]\ttrain-rmse:2849.189209 \n",
      "[88]\ttrain-rmse:2848.616699 \n",
      "[89]\ttrain-rmse:2846.478027 \n",
      "[90]\ttrain-rmse:2845.638916 \n",
      "[91]\ttrain-rmse:2844.796143 \n",
      "[92]\ttrain-rmse:2844.025391 \n",
      "[93]\ttrain-rmse:2843.437500 \n",
      "[94]\ttrain-rmse:2842.331299 \n",
      "[95]\ttrain-rmse:2839.847168 \n",
      "[96]\ttrain-rmse:2839.425537 \n",
      "[97]\ttrain-rmse:2838.491455 \n",
      "[98]\ttrain-rmse:2837.852295 \n",
      "[99]\ttrain-rmse:2836.847900 \n",
      "[100]\ttrain-rmse:2835.877441 \n",
      "[101]\ttrain-rmse:2835.311523 \n",
      "[102]\ttrain-rmse:2835.081055 \n",
      "[103]\ttrain-rmse:2834.343018 \n",
      "[104]\ttrain-rmse:2832.133057 \n",
      "[105]\ttrain-rmse:2831.510986 \n",
      "[106]\ttrain-rmse:2830.952637 \n",
      "[107]\ttrain-rmse:2830.593994 \n",
      "[108]\ttrain-rmse:2830.254150 \n",
      "[109]\ttrain-rmse:2829.472656 \n",
      "[110]\ttrain-rmse:2829.264648 \n",
      "[111]\ttrain-rmse:2829.089111 \n",
      "[112]\ttrain-rmse:2828.937744 \n",
      "[113]\ttrain-rmse:2828.138428 \n",
      "[114]\ttrain-rmse:2827.888672 \n",
      "[115]\ttrain-rmse:2826.825439 \n",
      "[116]\ttrain-rmse:2826.570068 \n",
      "[117]\ttrain-rmse:2825.211426 \n",
      "[118]\ttrain-rmse:2824.728271 \n",
      "[119]\ttrain-rmse:2824.013184 \n",
      "[120]\ttrain-rmse:2823.203857 \n",
      "[121]\ttrain-rmse:2822.429443 \n",
      "[122]\ttrain-rmse:2821.886230 \n",
      "[123]\ttrain-rmse:2821.607422 \n",
      "[124]\ttrain-rmse:2821.072266 \n",
      "[125]\ttrain-rmse:2820.879639 \n",
      "[126]\ttrain-rmse:2820.562500 \n",
      "[127]\ttrain-rmse:2820.264648 \n",
      "[128]\ttrain-rmse:2820.040527 \n",
      "[129]\ttrain-rmse:2819.229248 \n",
      "[130]\ttrain-rmse:2818.506104 \n",
      "[131]\ttrain-rmse:2818.134766 \n",
      "[132]\ttrain-rmse:2817.764160 \n",
      "[133]\ttrain-rmse:2817.345947 \n",
      "[134]\ttrain-rmse:2816.870850 \n",
      "[135]\ttrain-rmse:2816.503906 \n",
      "[136]\ttrain-rmse:2816.306641 \n",
      "[137]\ttrain-rmse:2815.859375 \n",
      "[138]\ttrain-rmse:2815.123047 \n",
      "[139]\ttrain-rmse:2814.726807 \n",
      "[140]\ttrain-rmse:2814.588135 \n",
      "[141]\ttrain-rmse:2814.049805 \n",
      "[142]\ttrain-rmse:2813.735840 \n",
      "[143]\ttrain-rmse:2812.849365 \n",
      "[144]\ttrain-rmse:2812.006836 \n",
      "[145]\ttrain-rmse:2811.182617 \n",
      "[146]\ttrain-rmse:2810.020020 \n",
      "[147]\ttrain-rmse:2809.578125 \n",
      "[148]\ttrain-rmse:2808.655273 \n",
      "[149]\ttrain-rmse:2808.332031 \n",
      "[150]\ttrain-rmse:2807.272461 \n",
      "[151]\ttrain-rmse:2807.041748 \n",
      "[152]\ttrain-rmse:2806.684814 \n",
      "[153]\ttrain-rmse:2805.889893 \n",
      "[154]\ttrain-rmse:2805.733887 \n",
      "[155]\ttrain-rmse:2804.589355 \n",
      "[156]\ttrain-rmse:2804.354004 \n",
      "[157]\ttrain-rmse:2804.235352 \n",
      "[158]\ttrain-rmse:2804.017822 \n",
      "[159]\ttrain-rmse:2803.603760 \n",
      "[160]\ttrain-rmse:2803.187500 \n",
      "[161]\ttrain-rmse:2803.084473 \n",
      "[162]\ttrain-rmse:2802.904541 \n",
      "[163]\ttrain-rmse:2802.104492 \n",
      "[164]\ttrain-rmse:2801.928467 \n",
      "[165]\ttrain-rmse:2801.584717 \n",
      "[166]\ttrain-rmse:2800.633545 \n",
      "[167]\ttrain-rmse:2800.466797 \n",
      "[168]\ttrain-rmse:2800.194092 \n",
      "[169]\ttrain-rmse:2799.913818 \n",
      "[170]\ttrain-rmse:2799.761963 \n",
      "[171]\ttrain-rmse:2799.510742 \n",
      "[172]\ttrain-rmse:2799.194092 \n",
      "[173]\ttrain-rmse:2798.359375 \n",
      "[174]\ttrain-rmse:2798.106689 \n",
      "[175]\ttrain-rmse:2797.815430 \n",
      "[176]\ttrain-rmse:2797.447754 \n",
      "[177]\ttrain-rmse:2796.652344 \n",
      "[178]\ttrain-rmse:2796.055420 \n",
      "[179]\ttrain-rmse:2795.868652 \n",
      "[180]\ttrain-rmse:2795.707275 \n",
      "[181]\ttrain-rmse:2795.079102 \n",
      "[182]\ttrain-rmse:2794.919434 \n",
      "[183]\ttrain-rmse:2794.277100 \n",
      "[184]\ttrain-rmse:2794.104980 \n",
      "[185]\ttrain-rmse:2793.529785 \n",
      "[186]\ttrain-rmse:2792.515381 \n",
      "[187]\ttrain-rmse:2791.234375 \n",
      "[188]\ttrain-rmse:2789.792725 \n",
      "[189]\ttrain-rmse:2789.733398 \n",
      "[190]\ttrain-rmse:2788.986816 \n",
      "[191]\ttrain-rmse:2788.881592 \n",
      "[192]\ttrain-rmse:2788.325684 \n",
      "[193]\ttrain-rmse:2787.933350 \n",
      "[194]\ttrain-rmse:2787.274414 \n",
      "[195]\ttrain-rmse:2787.147705 \n",
      "[196]\ttrain-rmse:2786.094971 \n",
      "[197]\ttrain-rmse:2784.983887 \n",
      "[198]\ttrain-rmse:2784.798340 \n",
      "[199]\ttrain-rmse:2784.538574 \n",
      "[200]\ttrain-rmse:2784.371826 \n",
      "[201]\ttrain-rmse:2783.842285 \n",
      "[202]\ttrain-rmse:2783.020020 \n",
      "[203]\ttrain-rmse:2782.749023 \n",
      "[204]\ttrain-rmse:2781.847656 \n",
      "[205]\ttrain-rmse:2781.718262 \n",
      "[206]\ttrain-rmse:2781.267090 \n",
      "[207]\ttrain-rmse:2780.862305 \n",
      "[208]\ttrain-rmse:2780.405029 \n",
      "[209]\ttrain-rmse:2779.885498 \n",
      "[210]\ttrain-rmse:2779.080566 \n",
      "[211]\ttrain-rmse:2778.683594 \n",
      "[212]\ttrain-rmse:2778.411133 \n",
      "[213]\ttrain-rmse:2778.188965 \n",
      "[214]\ttrain-rmse:2778.106201 \n",
      "[215]\ttrain-rmse:2777.847656 \n",
      "[216]\ttrain-rmse:2777.478027 \n",
      "[217]\ttrain-rmse:2777.268311 \n",
      "[218]\ttrain-rmse:2776.912598 \n",
      "[219]\ttrain-rmse:2776.013672 \n",
      "[220]\ttrain-rmse:2775.876709 \n",
      "[221]\ttrain-rmse:2775.472168 \n",
      "[222]\ttrain-rmse:2775.043457 \n",
      "[223]\ttrain-rmse:2774.271973 \n",
      "[224]\ttrain-rmse:2773.826904 \n",
      "[225]\ttrain-rmse:2773.649170 \n",
      "[226]\ttrain-rmse:2773.305664 \n",
      "[227]\ttrain-rmse:2773.243408 \n",
      "[228]\ttrain-rmse:2773.086670 \n",
      "[229]\ttrain-rmse:2772.853516 \n",
      "[230]\ttrain-rmse:2772.503174 \n",
      "[231]\ttrain-rmse:2772.354248 \n",
      "[232]\ttrain-rmse:2772.110596 \n",
      "[233]\ttrain-rmse:2771.765625 \n",
      "[234]\ttrain-rmse:2771.540771 \n",
      "[235]\ttrain-rmse:2771.372559 \n",
      "[236]\ttrain-rmse:2771.153564 \n",
      "[237]\ttrain-rmse:2771.011719 \n",
      "[238]\ttrain-rmse:2770.794922 \n",
      "[239]\ttrain-rmse:2770.639893 \n",
      "[240]\ttrain-rmse:2770.495117 \n",
      "[241]\ttrain-rmse:2770.120117 \n",
      "[242]\ttrain-rmse:2769.970947 \n",
      "[243]\ttrain-rmse:2769.861084 \n",
      "[244]\ttrain-rmse:2769.427979 \n",
      "[245]\ttrain-rmse:2769.305420 \n",
      "[246]\ttrain-rmse:2769.057129 \n",
      "[247]\ttrain-rmse:2768.582520 \n",
      "[248]\ttrain-rmse:2768.169922 \n",
      "[249]\ttrain-rmse:2768.086914 \n",
      "[250]\ttrain-rmse:2767.471924 \n",
      "[251]\ttrain-rmse:2766.626465 \n",
      "[252]\ttrain-rmse:2766.136230 \n",
      "[253]\ttrain-rmse:2765.958252 \n",
      "[254]\ttrain-rmse:2765.684082 \n",
      "[255]\ttrain-rmse:2765.522949 \n",
      "[256]\ttrain-rmse:2765.164307 \n",
      "[257]\ttrain-rmse:2765.061279 \n",
      "[258]\ttrain-rmse:2764.983154 \n",
      "[259]\ttrain-rmse:2764.826660 \n",
      "[260]\ttrain-rmse:2764.666016 \n",
      "[261]\ttrain-rmse:2764.419678 \n",
      "[262]\ttrain-rmse:2764.101807 \n",
      "[263]\ttrain-rmse:2763.889404 \n",
      "[264]\ttrain-rmse:2763.413818 \n",
      "[265]\ttrain-rmse:2762.255615 \n",
      "[266]\ttrain-rmse:2762.179688 \n",
      "[267]\ttrain-rmse:2762.020996 \n",
      "[268]\ttrain-rmse:2761.452881 \n",
      "[269]\ttrain-rmse:2761.326416 \n",
      "[270]\ttrain-rmse:2761.173340 \n",
      "[271]\ttrain-rmse:2761.068604 \n",
      "[272]\ttrain-rmse:2760.968018 \n",
      "[273]\ttrain-rmse:2760.782715 \n",
      "[274]\ttrain-rmse:2760.544434 \n",
      "[275]\ttrain-rmse:2760.394287 \n",
      "[276]\ttrain-rmse:2760.306641 \n",
      "[277]\ttrain-rmse:2759.973633 \n",
      "[278]\ttrain-rmse:2759.275146 \n",
      "[279]\ttrain-rmse:2758.946045 \n",
      "[280]\ttrain-rmse:2758.506104 \n",
      "[281]\ttrain-rmse:2758.411377 \n",
      "[282]\ttrain-rmse:2758.312256 \n",
      "[283]\ttrain-rmse:2758.173096 \n",
      "[284]\ttrain-rmse:2757.941162 \n",
      "[285]\ttrain-rmse:2757.547119 \n",
      "[286]\ttrain-rmse:2757.463135 \n",
      "[287]\ttrain-rmse:2757.285645 \n",
      "[288]\ttrain-rmse:2757.188477 \n",
      "[289]\ttrain-rmse:2756.898682 \n",
      "[290]\ttrain-rmse:2756.663330 \n",
      "[291]\ttrain-rmse:2756.548584 \n",
      "[292]\ttrain-rmse:2756.298828 \n",
      "[293]\ttrain-rmse:2755.986328 \n",
      "[294]\ttrain-rmse:2755.702881 \n",
      "[295]\ttrain-rmse:2755.576904 \n",
      "[296]\ttrain-rmse:2755.279297 \n",
      "[297]\ttrain-rmse:2755.120361 \n",
      "[298]\ttrain-rmse:2754.938477 \n",
      "[299]\ttrain-rmse:2754.816162 \n",
      "[300]\ttrain-rmse:2754.756836 \n",
      "[301]\ttrain-rmse:2754.103271 \n",
      "[302]\ttrain-rmse:2753.650391 \n",
      "[303]\ttrain-rmse:2753.207520 \n",
      "[304]\ttrain-rmse:2752.353516 \n",
      "[305]\ttrain-rmse:2752.241943 \n",
      "[306]\ttrain-rmse:2751.770264 \n",
      "[307]\ttrain-rmse:2751.486572 \n",
      "[308]\ttrain-rmse:2751.247070 \n",
      "[309]\ttrain-rmse:2751.118408 \n",
      "[310]\ttrain-rmse:2750.675293 \n",
      "[311]\ttrain-rmse:2750.414307 \n",
      "[312]\ttrain-rmse:2749.928223 \n",
      "[313]\ttrain-rmse:2749.683838 \n",
      "[314]\ttrain-rmse:2749.369141 \n",
      "[315]\ttrain-rmse:2749.127197 \n",
      "[316]\ttrain-rmse:2748.889648 \n",
      "[317]\ttrain-rmse:2748.648682 \n",
      "[318]\ttrain-rmse:2748.579346 \n",
      "[319]\ttrain-rmse:2748.066406 \n",
      "[320]\ttrain-rmse:2747.777100 \n",
      "[321]\ttrain-rmse:2747.458496 \n",
      "[322]\ttrain-rmse:2746.931641 \n",
      "[323]\ttrain-rmse:2746.878174 \n",
      "[324]\ttrain-rmse:2746.765137 \n",
      "[325]\ttrain-rmse:2746.378174 \n",
      "[326]\ttrain-rmse:2746.166992 \n",
      "[327]\ttrain-rmse:2746.050781 \n",
      "[328]\ttrain-rmse:2745.924805 \n",
      "[329]\ttrain-rmse:2745.795166 \n",
      "[330]\ttrain-rmse:2745.600830 \n",
      "[331]\ttrain-rmse:2745.461914 \n",
      "[332]\ttrain-rmse:2745.250488 \n",
      "[333]\ttrain-rmse:2745.159912 \n",
      "[334]\ttrain-rmse:2744.930664 \n",
      "[335]\ttrain-rmse:2744.311279 \n",
      "[336]\ttrain-rmse:2744.039551 \n",
      "[337]\ttrain-rmse:2743.794189 \n",
      "[338]\ttrain-rmse:2743.523193 \n",
      "[339]\ttrain-rmse:2743.436768 \n",
      "[340]\ttrain-rmse:2743.239258 \n",
      "[341]\ttrain-rmse:2742.965088 \n",
      "[342]\ttrain-rmse:2742.842285 \n",
      "[343]\ttrain-rmse:2742.646729 \n",
      "[344]\ttrain-rmse:2742.542725 \n",
      "[345]\ttrain-rmse:2742.379395 \n",
      "[346]\ttrain-rmse:2742.201904 \n",
      "[347]\ttrain-rmse:2742.022705 \n",
      "[348]\ttrain-rmse:2741.913818 \n",
      "[349]\ttrain-rmse:2741.727539 \n",
      "[350]\ttrain-rmse:2741.617920 \n",
      "[351]\ttrain-rmse:2741.174316 \n",
      "[352]\ttrain-rmse:2741.103027 \n",
      "[353]\ttrain-rmse:2740.991455 \n",
      "[354]\ttrain-rmse:2740.695557 \n",
      "[355]\ttrain-rmse:2740.104492 \n",
      "[356]\ttrain-rmse:2740.019775 \n",
      "[357]\ttrain-rmse:2739.833252 \n",
      "[358]\ttrain-rmse:2739.665283 \n",
      "[359]\ttrain-rmse:2739.439209 \n",
      "[360]\ttrain-rmse:2739.369629 \n",
      "[361]\ttrain-rmse:2739.255859 \n",
      "[362]\ttrain-rmse:2739.046143 \n",
      "[363]\ttrain-rmse:2738.864990 \n",
      "[364]\ttrain-rmse:2738.632324 \n",
      "[365]\ttrain-rmse:2738.310303 \n",
      "[366]\ttrain-rmse:2738.222900 \n",
      "[367]\ttrain-rmse:2738.077637 \n",
      "[368]\ttrain-rmse:2738.002930 \n",
      "[369]\ttrain-rmse:2737.825928 \n",
      "[370]\ttrain-rmse:2737.607422 \n",
      "[371]\ttrain-rmse:2737.542480 \n",
      "[372]\ttrain-rmse:2737.481934 \n",
      "[373]\ttrain-rmse:2737.226318 \n",
      "[374]\ttrain-rmse:2737.081299 \n",
      "[375]\ttrain-rmse:2736.871826 \n",
      "[376]\ttrain-rmse:2736.708496 \n",
      "[377]\ttrain-rmse:2736.487305 \n",
      "[378]\ttrain-rmse:2736.064453 \n",
      "[379]\ttrain-rmse:2735.864014 \n",
      "[380]\ttrain-rmse:2735.742920 \n",
      "[381]\ttrain-rmse:2735.558838 \n",
      "[382]\ttrain-rmse:2735.471924 \n",
      "[383]\ttrain-rmse:2735.309326 \n",
      "[384]\ttrain-rmse:2735.210693 \n",
      "[385]\ttrain-rmse:2734.994629 \n",
      "[386]\ttrain-rmse:2734.627686 \n",
      "[387]\ttrain-rmse:2734.488525 \n",
      "[388]\ttrain-rmse:2734.328369 \n",
      "[389]\ttrain-rmse:2734.158203 \n",
      "[390]\ttrain-rmse:2733.983887 \n",
      "[391]\ttrain-rmse:2733.786621 \n",
      "[392]\ttrain-rmse:2733.386963 \n",
      "[393]\ttrain-rmse:2733.264893 \n",
      "[394]\ttrain-rmse:2733.002930 \n",
      "[395]\ttrain-rmse:2732.957764 \n",
      "[396]\ttrain-rmse:2732.736572 \n",
      "[397]\ttrain-rmse:2732.672852 \n",
      "[398]\ttrain-rmse:2732.558838 \n",
      "[399]\ttrain-rmse:2732.441650 \n",
      "[400]\ttrain-rmse:2732.048828 \n",
      "[401]\ttrain-rmse:2731.918945 \n",
      "[402]\ttrain-rmse:2731.735596 \n",
      "[403]\ttrain-rmse:2731.237061 \n",
      "[404]\ttrain-rmse:2731.000977 \n",
      "[405]\ttrain-rmse:2730.650635 \n",
      "[406]\ttrain-rmse:2730.553955 \n",
      "[407]\ttrain-rmse:2730.394043 \n",
      "[408]\ttrain-rmse:2730.130127 \n",
      "[409]\ttrain-rmse:2730.060303 \n",
      "[410]\ttrain-rmse:2729.756836 \n",
      "[411]\ttrain-rmse:2729.609863 \n",
      "[412]\ttrain-rmse:2729.545410 \n",
      "[413]\ttrain-rmse:2729.453857 \n",
      "[414]\ttrain-rmse:2729.222656 \n",
      "[415]\ttrain-rmse:2728.693848 \n",
      "[416]\ttrain-rmse:2728.195068 \n",
      "[417]\ttrain-rmse:2728.076416 \n",
      "[418]\ttrain-rmse:2727.982178 \n",
      "[419]\ttrain-rmse:2727.766602 \n",
      "[420]\ttrain-rmse:2727.581055 \n",
      "[421]\ttrain-rmse:2727.398682 \n",
      "[422]\ttrain-rmse:2727.342041 \n",
      "[423]\ttrain-rmse:2727.172119 \n",
      "[424]\ttrain-rmse:2726.640869 \n",
      "[425]\ttrain-rmse:2726.454102 \n",
      "[426]\ttrain-rmse:2726.153564 \n",
      "[427]\ttrain-rmse:2726.072754 \n",
      "[428]\ttrain-rmse:2725.892334 \n",
      "[429]\ttrain-rmse:2725.660400 \n",
      "[430]\ttrain-rmse:2725.241211 \n",
      "[431]\ttrain-rmse:2724.964600 \n",
      "[432]\ttrain-rmse:2724.865723 \n",
      "[433]\ttrain-rmse:2724.680664 \n",
      "[434]\ttrain-rmse:2724.523682 \n",
      "[435]\ttrain-rmse:2724.411133 \n",
      "[436]\ttrain-rmse:2724.323730 \n",
      "[437]\ttrain-rmse:2724.077393 \n",
      "[438]\ttrain-rmse:2724.036621 \n",
      "[439]\ttrain-rmse:2723.770020 \n",
      "[440]\ttrain-rmse:2723.699951 \n",
      "[441]\ttrain-rmse:2723.171631 \n",
      "[442]\ttrain-rmse:2723.075928 \n",
      "[443]\ttrain-rmse:2722.941895 \n",
      "[444]\ttrain-rmse:2722.653809 \n",
      "[445]\ttrain-rmse:2722.432129 \n",
      "[446]\ttrain-rmse:2722.358398 \n",
      "[447]\ttrain-rmse:2722.291992 \n",
      "[448]\ttrain-rmse:2722.224121 \n",
      "[449]\ttrain-rmse:2721.778320 \n",
      "[450]\ttrain-rmse:2721.558350 \n",
      "[451]\ttrain-rmse:2721.424805 \n",
      "[452]\ttrain-rmse:2721.270996 \n",
      "[453]\ttrain-rmse:2721.150879 \n",
      "[454]\ttrain-rmse:2720.961670 \n",
      "[455]\ttrain-rmse:2720.764160 \n",
      "[456]\ttrain-rmse:2720.634277 \n",
      "[457]\ttrain-rmse:2720.316895 \n",
      "[458]\ttrain-rmse:2720.158936 \n",
      "[459]\ttrain-rmse:2719.941406 \n",
      "[460]\ttrain-rmse:2719.830566 \n",
      "[461]\ttrain-rmse:2719.562744 \n",
      "[462]\ttrain-rmse:2719.424805 \n",
      "[463]\ttrain-rmse:2719.119141 \n",
      "[464]\ttrain-rmse:2719.031738 \n",
      "[465]\ttrain-rmse:2718.770020 \n",
      "[466]\ttrain-rmse:2718.722412 \n",
      "[467]\ttrain-rmse:2718.561768 \n",
      "[468]\ttrain-rmse:2718.389648 \n",
      "[469]\ttrain-rmse:2718.188721 \n",
      "[470]\ttrain-rmse:2718.023193 \n",
      "[471]\ttrain-rmse:2717.988037 \n",
      "[472]\ttrain-rmse:2717.905518 \n",
      "[473]\ttrain-rmse:2717.856934 \n",
      "[474]\ttrain-rmse:2717.757080 \n",
      "[475]\ttrain-rmse:2717.676514 \n",
      "[476]\ttrain-rmse:2717.416016 \n",
      "[477]\ttrain-rmse:2717.251953 \n",
      "[478]\ttrain-rmse:2717.083740 \n",
      "[479]\ttrain-rmse:2717.040283 \n",
      "[480]\ttrain-rmse:2716.974365 \n",
      "[481]\ttrain-rmse:2716.667725 \n",
      "[482]\ttrain-rmse:2716.635010 \n",
      "[483]\ttrain-rmse:2716.568604 \n",
      "[484]\ttrain-rmse:2716.357422 \n",
      "[485]\ttrain-rmse:2716.146484 \n",
      "[486]\ttrain-rmse:2716.068604 \n",
      "[487]\ttrain-rmse:2716.034668 \n",
      "[488]\ttrain-rmse:2715.923340 \n",
      "[489]\ttrain-rmse:2715.776123 \n",
      "[490]\ttrain-rmse:2715.600342 \n",
      "[491]\ttrain-rmse:2715.477295 \n",
      "[492]\ttrain-rmse:2715.271240 \n",
      "[493]\ttrain-rmse:2715.151367 \n",
      "[494]\ttrain-rmse:2715.063965 \n",
      "[495]\ttrain-rmse:2714.878174 \n",
      "[496]\ttrain-rmse:2714.663818 \n",
      "[497]\ttrain-rmse:2714.611572 \n",
      "[498]\ttrain-rmse:2714.428223 \n",
      "[499]\ttrain-rmse:2714.315918 \n",
      "[500]\ttrain-rmse:2714.062256 \n"
     ]
    }
   ],
   "source": [
    "model_tuned3 = xgboost(data = d_train,\n",
    "                      eta = 0.1,                     #controls the step size shrinkage, default=0.3\n",
    "                      nround = 500,                  #since we are decreasing the eta, we need high nround value\n",
    "                      objective = \"reg:linear\",\n",
    "                      max_depth = 10,                #reduces the depth of the trees to reduce complexity\n",
    "                      min_child_weight = 200,        #higher the value, more instances required to make a node\n",
    "                      gamma = 150,                   #minimum loss reduction required to make further partition of a leaf node\n",
    "                      subsample = 0.5,               #subsamples the instances in the training set by half\n",
    "                      colsample_bytree = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"RMSE3:  2783.1487690343\"\n",
      "[1] \"RMSE3 difference:  69.0865130343022\"\n"
     ]
    }
   ],
   "source": [
    "#Calculate the purchase price for testing data\n",
    "pred_purchase3 = predict(model_tuned3, d_test)\n",
    "\n",
    "#Calculate the RMSE observed\n",
    "purchase_err3 = pred_purchase3 - purchaseValues_Test\n",
    "rmse3 = sqrt(mean(purchase_err3^2))\n",
    "print(paste(\"RMSE3: \",rmse3))\n",
    "\n",
    "#Fetching the RMSE calculated in the last iteration\n",
    "eval_log = model_tuned3[4]\n",
    "log = eval_log[[1]]\n",
    "rmse_calc3 = log$train_rmse[log$iter==500]\n",
    "print(paste(\"RMSE3 difference: \", rmse3-rmse_calc3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the second model performed the best amongst the three models with the least difference in the RMSE as observed on training and testing data. Hence we'll proceed with that model. It should be noted that at this point we can try to lower the difference in the RMSE by using different parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "\n",
    "Now that we have trained our model, we can observe and evaluate it. One important insight that can be drawn from the use of XGBoost model is that which of the variables/features play an important role in prediction. Here's how it is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAC8VBMVEUAAAABAQECAgIDAwME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\nEBJgACEBBhASYAAhAQb8f5T5jI+AeWV3AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Matrix containing data on the importance of each variable\n",
    "importance_matrix <- xgb.importance(names(purchaseData_Matrix), model = model_tuned2)\n",
    "\n",
    "# and plot it!\n",
    "xgb.plot.importance(importance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from the importance_matrix plot that the variable \"Product_Category_1\" seems to be the most important variable, followed by the \"Product_Category_2\", \"Product_Category_3\", and \"User_ID\". This suggests that to improve the model's performance, we can incorporate feature engineering to emphasize on the purchasing pattern of a particular user.\n",
    "\n",
    "Moreover, with the information on the relationship of the customer or user and the product category, that he or she purchases more often, the organization can market their products much more effeciently, with effective results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
